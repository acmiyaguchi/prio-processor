#!/bin/bash
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.

# This scripts defines the batched processing pipeline for use on GCP.

set -euo pipefail
set -x

# Re-export variables for GNU parallel 20160222. Newer versions of parallel
# contain a env_parallel script for exporting the current environment.

# Avoid printing sensitive environment data into the logs
set +x
export SHARED_SECRET=${SHARED_SECRET?}
export PRIVATE_KEY_HEX=${PRIVATE_KEY_HEX?}
set -x

export N_DATA=${N_DATA?}
export BATCH_ID=${BATCH_ID?}
export SERVER_ID=${SERVER_ID?}
export PUBLIC_KEY_HEX_INTERNAL=${PUBLIC_KEY_HEX_INTERNAL?}
export PUBLIC_KEY_HEX_EXTERNAL=${PUBLIC_KEY_HEX_EXTERNAL?}

: "${BUCKET_INTERNAL_PRIVATE?}"
: "${BUCKET_INTERNAL_SHARED?}"
: "${BUCKET_EXTERNAL_SHARED?}"
: "${GCP_CREDENTIALS?}"

RETRY_LIMIT=5
BACKOFF_EXPONENT=2


function create_folders() {
    mkdir -p raw
    mkdir -p intermediate/internal/verify1
    mkdir -p intermediate/external/verify1
    mkdir -p intermediate/internal/verify2
    mkdir -p intermediate/external/verify2
    mkdir -p intermediate/internal/aggregate
    mkdir -p intermediate/external/aggregate
    mkdir -p processed
}

# Copy data generated by a processing step into the receiving bucket of the
# co-processing server. A _SUCCESS file is generated on a successful copy.
#
# Arguments:
#   $1 - relative path to output for a processing stage
#   $2 - relative path to the external bucket of the external server
#
# Environment:
#   BUCKET_EXTERNAL_SHARED - The bucket of the external server
function send_output_external() {
    local output_internal=$1
    local output_external=$2
    local path="gs://${BUCKET_EXTERNAL_SHARED}/${output_external}"
    gsutil -m rsync -r "${output_internal}/" "${path}/"
    touch _SUCCESS
    gsutil cp _SUCCESS "${path}/"
}


# Copy local data to a remote bucket that is accessible to the current server.
#
# Arguments:
#   $1 - relative path to a folder containing data
#
# Environment:
#   TARGET - The minio host
#   BUCKET_INTERNAL_PRIVATE - The bucket pointing to this server's data
function send_output_internal() {
    local output=$1
    local path="gs://${BUCKET_INTERNAL_PRIVATE}/$output"
    gsutil -m rsync -r "${output}/" "${path}/"
    touch _SUCCESS
    gsutil cp _SUCCESS "${path}/"
}

# Wait for a completed batch of data, signaled by the appearance of a _SUCCESS
# file.
#
# Arguments:
#   $1 - Absolute path to a file
function wait_for_data() {
    local path=$1
    set +e
    max_retries=$RETRY_LIMIT
    retries=0
    backoff=2
    while ! gsutil -q stat "${path}"; do
        sleep ${backoff};
        ((backoff *= BACKOFF_EXPONENT))
        ((retries++))
        if [[ "${retries}" -gt "${max_retries}" ]]; then
            echo "Reached the maximum number of retries."
            exit 1
        fi
    done
    set -e
}


# Poll for a success file and then copy the appropriate files locally
#
# Arguments:
#   $1 - Input bucket
#   $2 - Path relative to the input bucket
function fetch_input_blocked() {
    local bucket=$1
    local input=$2
    local path="gs://${bucket}/${input}"
    wait_for_data "${path}/_SUCCESS"
    gsutil -m rsync -r "${path}/" "${input}/"
}

# Wait for data to be written to the internal private bucket before copying.
#
# Arguments:
#   $1 - The path relative to the internal private bucket
#
# Environment:
#   BUCKET_INTERNAL_PRIVATE
function fetch_input_private() {
    local input=$1
    fetch_input_blocked "${BUCKET_INTERNAL_PRIVATE}" "${input}"
}


# Wait for data to be written to the internal shared bucket before copying.
#
# Arguments:
#   $1 - The path relative to the internal shared bucket
#
# Environment:
#   BUCKET_INTERNAL_SHARED
function fetch_input_shared() {
    local input=$1
    fetch_input_blocked "${BUCKET_INTERNAL_SHARED}" "${input}"
}


# List partitions that are ready for processing.
#
# Arguments:
#   $1 - relative path to a folder containing data
function list_partitions() {
    local input=$1
    find "${input}" -type f -not -name "_SUCCESS" -printf "%f\n"
}


# Read the shares for this server and start verification with the external
# server.
function verify1() {
    export input="raw"
    export output_internal="intermediate/internal/verify1"
    export output_external="intermediate/external/verify1"

    fetch_input_private ${input}

    function process() {
        local filename=$1
        prio verify1 \
            --input "${input}/${filename}" \
            --output ${output_internal}
    }
    export -f process
    parallel process ::: "$(list_partitions ${input})"

    send_output_external ${output_internal} ${output_external}
}


# Verify that the shares are well-formed by proving a secret-shared
# non-interactive proof.
function verify2() {
    export input="raw"
    export input_internal="intermediate/internal/verify1"
    export input_external="intermediate/external/verify1"
    export output_internal="intermediate/internal/verify2"
    export output_external="intermediate/external/verify2"

    fetch_input_shared ${input_external}

    function process() {
        local filename=$1
        prio verify2 \
            --input "${input}/${filename}" \
            --input-internal "${input_internal}/${filename}" \
            --input-external "${input_external}/${filename}" \
            --output ${output_internal}
    }
    export -f process
    parallel process ::: "$(list_partitions ${input_internal})"

    send_output_external ${output_internal} ${output_external}
}


# Accumulate well-formed shares.
function aggregate() {
    export input="raw"
    export input_internal="intermediate/internal/verify2"
    export input_external="intermediate/external/verify2"
    export output_internal="intermediate/internal/aggregate"
    export output_external="intermediate/external/aggregate"

    fetch_input_shared ${input_external}

    function process() {
        local filename=$1
        prio aggregate \
            --input "${input}/${filename}" \
            --input-internal "${input_internal}/${filename}" \
            --input-external "${input_external}/${filename}" \
            --output ${output_internal}
    }
    export -f process
    parallel process ::: "$(list_partitions ${input_internal})"

    send_output_external ${output_internal} ${output_external}
}


# Publish the aggregated shares.
function publish() {
    export input_internal="intermediate/internal/aggregate"
    export input_external="intermediate/external/aggregate"
    export output="processed"

    fetch_input_shared ${input_external}

    function process() {
        local filename=$1
        prio publish \
            --input-internal "${input_internal}/${filename}" \
            --input-external "${input_external}/${filename}" \
            --output ${output}
    }
    export -f process
    parallel process ::: "$(list_partitions ${input_internal})"

    jq -c '.' ${output}/*.ndjson

    send_output_internal ${output}
}

function authenticate() {
    gcloud auth activate-service-account --key-file "${GCP_CREDENTIALS}"
}

function main() {
    authenticate
    cd /tmp && create_folders

    verify1
    verify2
    aggregate
    publish
}

main "$@"
