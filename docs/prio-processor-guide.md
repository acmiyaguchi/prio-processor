# Prio Processor

`mozilla/prio-processor` is a container application that implements the privacy
and correctness guarantees of Prio, a privacy-preserving aggregation system. The
processor interoperates with the Firefox Data Platform by an agreed convention
of data exchange across cloud storage.

The initial release (v1.0) contain an automated workflow for batched processing
of ["prio"
pings](https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/telemetry/data/prio-ping.html)
that are ingested via
[mozilla/gcp-ingestion](https://github.com/mozilla/gcp-ingestion).


This processor aggregates [Origin Telemetry
pings](https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/telemetry/collection/origin.html)
configured to measure the occurrences of compatibility exemptions across content
blocklists in Firefox in pre-release channels.

## Container application overview

The container can be built from source using Docker. This can be run locally or
in a container service such as Google Kubernetes Engine (GKE).

To build the container:

```bash
make build
```

This will generate two images that are ready to use. The development image is
configured to run unit and integration tests, while the production image will
initialize the single trigger, batched-processing mode.

```bash
# run the tests
docker run prio:dev

# start a shell session
# --interactive --tty
docker run -it prio:dev bash

# start the server
docker run prio:latest
```

See the prio-processor README for more details about the development
environment.

### python-libprio

`python-libprio` is a Python3 extension module that wraps the Prio routines. It
implements a SWIG interface for building Python wrappers.

The wrapper generated by the SWIG interface file mirrors the structure of code
in "mprio.h".

```python
from prio.libprio import *


data_in = ...
server = ...

verifier = PrioVerifier_new(server)
packet = PrioPacketVerify1_new()

PrioVerifier_set_data(verifier, data)
PrioPacketVerify1_set_data(packet, verifier)

data_out = PrioPacketVerify1_write(packet)
```

The `prio` module provides an API that takes advantage of Python capabilities.

```python
from prio.prio import *
import pickle


data_in = ...
server = ...

with Prio() as p:
    verifier = p.Verifier(server, data_in)
    data_out = pickle.dumps(verifier.create_verify1())
```

These libraries are used to build up a series of examples that include asyncio,
multiprocessing, and docker-compose usage.

### The `prio` cli tool

The `prio` command-line interface (CLI) that can be automated in shell script of
choice.

```bash
$ prio --help

Usage: prio [OPTIONS] COMMAND [ARGS]...

  Command line utility for prio.

Options:
  --help  Show this message and exit.

Commands:
  aggregate      Generate an aggregate share from a batch of verified SNIPs
  encode-shares  Encode JSON bit-vector into base64-encoded shares.
  keygen         Generate a curve25519 key pair as json.
  publish        Generate a final aggregate and remap data to a content...
  shared-seed    Generate a shared server secret in base64.
  verify1        Decode a batch of shares
  verify2        Verify a batch of SNIPs
```

The CLI uses `click` for option parsing and command structure. The `jq` tool is
the primary tool to diagnose data.

```sh
# Create a new JSON key and store it to a file.
$ prio keygen > keyfile

# Create 
$ jq 'keys'  keyfile
[
  "private_key",
  "public_key"
]

$ jq -r '.public_key' keyfile
"AF90AA5CE9B8D34D1777770C64F2CB44739EF625C61E263FB308DA85BE4D2016"
```

The `prio` commands accept arguments either the environment or the command-line.
This example of creating a shared proof utilizes `jq` and `prio` in shell.

```sh
#!/bin/sh
export PUBLIC_KEY_HEX_INTERNAL=$(prio keygen | jq -r '.public_key')
export PUBLIC_KEY_HEX_EXTERNAL=$(prio keygen | jq -r '.public_key')

cd /tmp
mkdir -p data/a data/b

n_data=10

# an array with ten `1`s
python3 -c "print([1]*${n_data})" > data/part-0.json

prio encode-shares \
    --n-data ${n_data} \
    --batch-id "testing.test-0" \
    --input /tmp/data/part-0.json \
    --output-A /tmp/data/a \
    --output-B /tmp/data/b

cd -
```

Data looks like this:

```json
{
  "id": "7dc16f11-87bf-4631-944a-4b63a8646502",
  "payload": "BvSRhK0vxVp+/6plOy+lVw9h1soYKA0N/QB130p7qV2CXai9tkNjqZQAihhm..."
}
```

### Parallel processing

The `prio-processor run-cycle` script uses GNU Parallel to maximize resource
utilization. The `prio-processor staging` command will transform data from the
"prio" ping into `(id, packet)` records, range partitioned by their batch
identifiers.

```bash
# re-export the environment

function list_partitions() { ... }
function verify1() { ... }

# export the function
export -f verify1
parallel verify1 :: $(list_partitions)

```

 
## Data Exchange

### Ranged Partitioning

Data is bundled into partitions where partitions have been assigned based on
matching ids.

```sh
├── _SUCCESS
└── submission_date=2019-06-26
    ├── server_id=a
    │   ├── batch_id=content.blocking_blocked_TESTONLY-0
    │   │   ├── part-00000-6adba759-6e58-4092-8120-6331705e2e46.c000.json
    │   │   └── part-00001-6adba759-6e58-4092-8120-6331705e2e46.c000.json
    │   └── batch_id=content.blocking_blocked_TESTONLY-1
    │       ├── part-00002-6adba759-6e58-4092-8120-6331705e2e46.c000.json
    │       └── part-00003-6adba759-6e58-4092-8120-6331705e2e46.c000.json
    └── server_id=b
        ├── batch_id=content.blocking_blocked_TESTONLY-0
        │   ├── part-00000-6adba759-6e58-4092-8120-6331705e2e46.c000.json
        │   └── part-00001-6adba759-6e58-4092-8120-6331705e2e46.c000.json
        └── batch_id=content.blocking_blocked_TESTONLY-1
            ├── part-00002-6adba759-6e58-4092-8120-6331705e2e46.c000.json
            └── part-00003-6adba759-6e58-4092-8120-6331705e2e46.c000.json
```

### Filesystem exchange

The co-processors share data by using cloud storage. Each storage unit is
separated by path hierarchy and permissions implemented by the filesystem. The
path encodes various metadata.

| Attribute       | Purpose                                                 |
|-----------------|---------------------------------------------------------|
| submission-date | The date of the batch processing submission.            |
| server-id       | The recipient of the flattened and partitioned shares.  |
| batch-id        | Used to identify the encoding and size of the data.     |
| part-id         | Identify a partition in an ordered range of partitions. |

The overall view of the path hierarchy.

```bash
filesystem
├── server_a
│   ├── intermediate
│   │   ├── external
│   │   │   ├── aggregate
│   │   │   ├── verify1
│   │   │   └── verify2
│   │   └── internal
│   │       ├── aggregate
│   │       ├── verify1
│   │       └── verify2
│   ├── processed
│   └── raw
└── server_b
    ├── intermediate
    │   ├── external
    │   │   ├── aggregate
    │   │   ├── verify1
    │   │   └── verify2
    │   └── internal
    │       ├── aggregate
    │       ├── verify1
    │       └── verify2
    ├── processed
    └── raw
```

The view of the paths when viewed from one project.

```bash
filesystem
├── server_a
│   ├── intermediate
│   │   ├── external
│   │   │   ├── aggregate
│   │   │   ├── verify1
│   │   │   └── verify2
│   │   └── internal
│   │       ├── aggregate
│   │       ├── verify1
│   │       └── verify2
│   ├── processed
│   └── raw
└── server_b
    └─── intermediate
        └─── external
            ├── aggregate
            ├── verify1
            └── verify2
```

### Triggering mechanism

The processor is designed for ad-hoc usage that can be scheduled externally. The
application starts up and waits for data in a specified location. When data is
signaled in the receiving bucket, it processes the partition at a time, and
writes it to the other server's receiving bucket. This process is repeated for
all of the stages involved in aggregation: `verify1`, `verify2`, `aggregate`,
and `publish`.

Once all stages are complete, the processor will terminate and clear the state
of the buckets.

#### Retries and exponential backoff

The following variables control exponential backoff:

```bash
RETRY_LIMIT
RETRY_DELAY
RETRY_BACKOFF_EXPONENT
```

### Configuring cloud storage

There is support for s3 and gcs storage via
[`gsutil`](https://cloud.google.com/storage/docs/interoperability).

### Scheduling

The staging frequency should match the processor job frequency. Both servers
should come online within the tolerances of the retry mechanism controled by the
`RETRY_*` variables.

## Suggested configuration

Partitions have been configured to contain an upper bound of 250mb per
partition. The machine running the processor should contain a large number of
cores (32+) to maximize resource utilization. There should be 1GB associated to
each core.
